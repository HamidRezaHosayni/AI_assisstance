import os
import json
import numpy as np
from PyPDF2 import PdfReader
from concurrent.futures import ThreadPoolExecutor
from request_ollama.ollama_api import OllamaAPI

class PDFSearcher:
    def __init__(self, pdf_folder: str = "search_pdf/pdf_files", embeddings_folder: str = "search_pdf/embeddings"):
        """
        Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ù„Ø§Ø³ Ø¬Ø³ØªØ¬ÙˆÚ¯Ø± PDF
        """
        self.pdf_folder = os.path.abspath(pdf_folder)
        self.embeddings_folder = os.path.abspath(embeddings_folder)
        os.makedirs(self.pdf_folder, exist_ok=True)
        os.makedirs(self.embeddings_folder, exist_ok=True)

        self.ollama_api = OllamaAPI()
        self.embedding_model = "nomic-embed-text:latest"  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±
        self.pdf_embeddings = {}

    def process_pdf(self, pdf_name: str):
        """
        Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© ÙØ§ÛŒÙ„ PDF Ùˆ Ø°Ø®ÛŒØ±Ù‡ embeddingâ€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ JSON
        """
        pdf_path = os.path.join(self.pdf_folder, pdf_name)
        embeddings_file = os.path.join(self.embeddings_folder, f"{os.path.splitext(pdf_name)[0]}.json")

        if os.path.exists(embeddings_file):
            print(f"Embeddingâ€ŒÙ‡Ø§ÛŒ ÙØ§ÛŒÙ„ {pdf_name} Ù‚Ø¨Ù„Ø§Ù‹ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.")
            return

        reader = PdfReader(pdf_path)
        chunks = []

        for page_num, page in enumerate(reader.pages, start=1):
            try:
                text = page.extract_text()
                if text and text.strip():
                    sentences = [s.strip() for s in text.split('.') if s.strip()]
                    current_chunk = ""
                    for sentence in sentences:
                        if len(current_chunk) + len(sentence) < 500:
                            current_chunk += " " + sentence
                        else:
                            chunks.append({'text': current_chunk.strip(), 'page': page_num})
                            current_chunk = sentence
                    if current_chunk:
                        chunks.append({'text': current_chunk.strip(), 'page': page_num})
                else:
                    print(f"Ù‡Ø´Ø¯Ø§Ø±: ØµÙØ­Ù‡ {page_num} Ø§Ø² ÙØ§ÛŒÙ„ {pdf_name} Ø­Ø§ÙˆÛŒ Ù…ØªÙ† Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ÛŒØ³Øª.")
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² ØµÙØ­Ù‡ {page_num} ÙØ§ÛŒÙ„ {pdf_name}: {e}")

        if chunks:
            chunk_embeddings = self._compute_embeddings_parallel(chunks)
            embeddings_to_save = {chunk['text']: embedding for chunk, embedding in zip(chunks, chunk_embeddings)}

            with open(embeddings_file, "w") as f:
                json.dump(embeddings_to_save, f, ensure_ascii=False, indent=4)
                print(f"Embeddingâ€ŒÙ‡Ø§ÛŒ ÙØ§ÛŒÙ„ {pdf_name} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
        else:
            print(f"Ù‡Ø´Ø¯Ø§Ø±: ÙØ§ÛŒÙ„ {pdf_name} Ø­Ø§ÙˆÛŒ Ù…ØªÙ† Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ÛŒØ³Øª.")

    def _compute_embeddings_parallel(self, chunks):
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Embeddingâ€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ
        """
        embeddings = []
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {executor.submit(self.ollama_api.get_embedding, chunk['text'], self.embedding_model): chunk for chunk in chunks}
            for future in futures:
                chunk = futures[future]
                try:
                    embedding = future.result()
                    if embedding:
                        embeddings.append(embedding)
                    else:
                        print(f"Ù‡Ø´Ø¯Ø§Ø±: embedding Ø¨Ø±Ø§ÛŒ Ù…ØªÙ† Ø²ÛŒØ± ØªÙˆÙ„ÛŒØ¯ Ù†Ø´Ø¯:\n{chunk['text'][:100]}...")
                        embeddings.append([0] * 384)  # Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ embedding Ù†Ø§Ù…ÙˆÙÙ‚
                except Exception as e:
                    print(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ embedding Ø¨Ø±Ø§ÛŒ Ù…ØªÙ† Ø²ÛŒØ±:\n{chunk['text'][:100]}...\nØ®Ø·Ø§: {e}")
                    embeddings.append([0] * 384)
        return embeddings

    def load_embeddings(self, pdf_name: str):
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ embeddingâ€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„ JSON
        """
        embeddings_file = os.path.join(self.embeddings_folder, f"{os.path.splitext(pdf_name)[0]}.json")
        if not os.path.exists(embeddings_file):
            print(f"ÙØ§ÛŒÙ„ embedding Ø¨Ø±Ø§ÛŒ {pdf_name} ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ PDF...")
            self.process_pdf(pdf_name)

        try:
            with open(embeddings_file, "r") as f:
                return json.load(f)
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ {embeddings_file}: {e}")
            return {}

    def search(self, query: str, pdf_name: str, top_k: int = 5, similarity_threshold: float = 0.7) -> list:
        """
        Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ø± ÛŒÚ© ÙØ§ÛŒÙ„ PDF Ùˆ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ÙÙ‚Ø· Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
        """
        query_embedding = self.ollama_api.get_embedding(query, self.embedding_model)
        embeddings = self.load_embeddings(pdf_name)

        results = []
        for text, embedding in embeddings.items():
            similarity = self._cosine_similarity(query_embedding, embedding)
            if similarity >= similarity_threshold:  # Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª
                results.append({'context': text, 'similarity': similarity})

        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]  # ÙÙ‚Ø· `top_k` Ù†ØªÛŒØ¬Ù‡ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯

    def get_relevant_context(self, query: str, pdf_name: str, max_chars: int = 1000) -> str:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ù…ØªÙ† Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
        """
        results = self.search(query, pdf_name, top_k=5, similarity_threshold=0.7)  # Ù…Ù‚Ø¯Ø§Ø± Ø´Ø¨Ø§Ù‡Øª ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡

        context = ""
        total_chars = 0  # Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø±Ø³Ø§Ù„ Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ Ø¯Ø§Ø¯Ù‡

        if results:
            print("\nÙ…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· ÛŒØ§ÙØª Ø´Ø¯Ù‡:")
            for result in results:
                text_chunk = result['context']
                similarity = result['similarity']

                if total_chars + len(text_chunk) <= max_chars:  # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª
                    context += text_chunk + "\n"
                    total_chars += len(text_chunk)

                    print(f"ğŸ“Œ Ù…ØªÙ† Ù…Ø±ØªØ¨Ø· (Similarity: {similarity:.2f}): {text_chunk[:100]}...")  # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´
                else:
                    break
        else:
            print("\nâš ï¸ Ù†ØªÛŒØ¬Ù‡ Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

        return context.strip()  # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ùˆ Ø¨Ø§Ø²Ú¯Ø´Øª ÙÙ‚Ø· Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…

    @staticmethod
    def _cosine_similarity(vec1, vec2):
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ
        """
        vec1, vec2 = np.array(vec1), np.array(vec2)
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))